{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e987dba-3901-41fc-83f4-b09f94f493e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"v20251219\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using JSON\n",
    "using ArchGDAL\n",
    "using Proj\n",
    "using Rasters\n",
    "using Base.Threads\n",
    "using JLD2\n",
    "using Lux\n",
    "using LuxCore\n",
    "using EasyHybrid\n",
    "using Optimisers\n",
    "using Statistics\n",
    "using Plots\n",
    "using Distributed\n",
    "using Parquet\n",
    "include(\"helpers.jl\")\n",
    "using .Helpers\n",
    "version = \"v20251219\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d5fc5-95e6-49e4-85ee-f772b83da1b0",
   "metadata": {},
   "source": [
    "## train production model with all available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0159b67-7786-4265-b16e-b19ac67fb072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "# load in predictors\n",
    "datafile = \"/mnt/tupi/HybridModeling/EasyDensity.jl/data/lucas_preprocessed_v20251125.csv\"\n",
    "oridf = CSV.read(datafile, DataFrame; normalizenames=true)\n",
    "predictors = Symbol.(names(oridf))[18:end-6]; # CHECK EVERY TIME \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d195b9d-436e-4687-8853-6b2ceefbf54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = (\n",
    "    SOCconc = (0.01f0, 0.0f0, 1.0f0),   # fraction\n",
    "    CF      = (0.15f0, 0.0f0, 1.0f0),   # fraction,\n",
    "    oBD     = (0.20f0, 0.05f0, 0.40f0),  # also NN learnt, g/cm3\n",
    "    mBD     = (1.20f0, 0.75f0, 2.0f0),  # NN leanrt\n",
    ")\n",
    "neural_param_names = [:SOCconc, :CF, :mBD, :oBD]\n",
    "forcing = Symbol[]\n",
    "targets = [:BD, :SOCconc, :SOCdensity, :CF]   \n",
    "\n",
    "hmb = constructHybridModel(\n",
    "    predictors,\n",
    "    forcing,\n",
    "    targets,\n",
    "    SOCD_model,\n",
    "    parameters,\n",
    "    neural_param_names,\n",
    "    [];\n",
    "    hidden_layers = [256, 128, 64, 32],\n",
    "    activation = gelu,\n",
    "    scale_nn_outputs = true,\n",
    "    input_batchnorm = false,\n",
    "    start_from_default = true\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c68567df-add7-4057-bd83-256559a269f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mMakie extension not loaded, no plots will be generated.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ EasyHybrid /opt/julia/packages/EasyHybrid/PjjBT/src/train.jl:156\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPlotting disabled.\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mCheck the saved output (.png, .mp4, .jld2) from training at: /mnt/tupi/HybridModeling/EasyDensity.jl-main/output_tmp\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mEarly stopping at epoch 19 with best validation loss wrt mse: 0.014769173349902721\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ EasyHybrid /opt/julia/packages/EasyHybrid/PjjBT/src/train.jl:319\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mReturning best model from epoch 4 of 200 epochs with best validation loss wrt mse: 0.014769173349902721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[38;5;6m  train_history\u001b[39m\u001b[33m: \u001b[39m\u001b[90m(20, 2)\u001b[39m\n",
       "    mse  (BD, SOCconc, SOCdensity, CF, mean)\n",
       "    r2   (BD, SOCconc, SOCdensity, CF, mean)\n",
       "\u001b[38;5;6m  val_history\u001b[39m\u001b[33m: \u001b[39m\u001b[90m(20, 2)\u001b[39m\n",
       "    mse  (BD, SOCconc, SOCdensity, CF, mean)\n",
       "    r2   (BD, SOCconc, SOCdensity, CF, mean)\n",
       "\u001b[38;5;6m  ps_history\u001b[39m\u001b[33m: \u001b[39m\u001b[90m(20, 2)\u001b[39m\n",
       "    ϕ        ()\n",
       "    monitor  (train, val)\n",
       "\u001b[38;5;6m  train_obs_pred\u001b[39m\u001b[33m: \u001b[39m\u001b[90m44894×9 DataFrame\u001b[39m\n",
       "\u001b[34m    \u001b[39mBD, SOCconc, SOCdensity, CF, index, BD_pred, SOCconc_pred, SOCdensity_pred, CF_pred\n",
       "\u001b[38;5;6m  val_obs_pred\u001b[39m\u001b[33m: \u001b[39m\u001b[90m11223×9 DataFrame\u001b[39m\n",
       "\u001b[34m    \u001b[39mBD, SOCconc, SOCdensity, CF, index, BD_pred, SOCconc_pred, SOCdensity_pred, CF_pred\n",
       "\u001b[38;5;6m  train_diffs\u001b[39m\u001b[33m: \u001b[39m\n",
       "\u001b[38;5;10m    oBD         \u001b[39m\u001b[90m(44894,)\u001b[39m\n",
       "\u001b[38;5;10m    mBD         \u001b[39m\u001b[90m(44894,)\u001b[39m\n",
       "    parameters  (SOCconc, CF, mBD, oBD)\n",
       "\u001b[38;5;6m  val_diffs\u001b[39m\u001b[33m: \u001b[39m\n",
       "\u001b[38;5;10m    oBD         \u001b[39m\u001b[90m(11223,)\u001b[39m\n",
       "\u001b[38;5;10m    mBD         \u001b[39m\u001b[90m(11223,)\u001b[39m\n",
       "    parameters  (SOCconc, CF, mBD, oBD)\n",
       "\u001b[38;5;6m  ps\u001b[39m\u001b[33m: \u001b[39m\n",
       "    ps  (layer_1, layer_2, layer_3, layer_4, layer_5, layer_6)\n",
       "\u001b[38;5;6m  st\u001b[39m\u001b[33m: \u001b[39m\n",
       "    st     (layer_1, layer_2, layer_3, layer_4, layer_5, layer_6)\n",
       "    fixed  ()\n",
       "\u001b[38;5;6m  best_epoch\u001b[39m\u001b[33m: \u001b[39m\n",
       "\u001b[38;5;6m  best_loss\u001b[39m\u001b[33m: \u001b[39m\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlt = train(\n",
    "    hmb, oridf, ();\n",
    "    nepochs = 200,\n",
    "    batchsize = 512,\n",
    "    opt = AdamW(0.0005),\n",
    "    training_loss = :mse,\n",
    "    loss_types = [:mse, :r2],\n",
    "    shuffleobs = true,\n",
    "    file_name = \"prod_SiNN.jld2\",\n",
    "    random_seed = 42,\n",
    "    patience = 15,\n",
    "    yscale = identity,\n",
    "    monitor_names = [:oBD, :mBD],\n",
    "    agg = mean,\n",
    "    return_model = :best,\n",
    "    show_progress = false,\n",
    "    plotting = false,\n",
    "    hybrid_name = \"model_prod_SiNN\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d14c74c-3f20-4eff-8468-973fdae7ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pss = rlt.ps\n",
    "stt = rlt.st\n",
    "@save \"./map/prod_SiNN_model_$(version).jld2\" hmb pss stt\n",
    "@save \"./data/predictors_$(version).jld2\" predictors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830fbb4b-b46d-4665-aa48-1bcacabc792f",
   "metadata": {},
   "source": [
    "## save the covariate scalers using before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692c15ae-b87e-4250-99ee-675d97c53248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62577, 422)\n",
      "(62199, 421)\n",
      "(62199, 422)\n",
      "(57343, 422)\n",
      "33487 CHELSA_swe_1981_2010_V_2_1\n",
      "(56117, 415)\n",
      "(56117, 380)\n"
     ]
    }
   ],
   "source": [
    "# ? move the `csv` file into the `BulkDSOC/data` folder (create folder)\n",
    "df_o = CSV.read(\"/mnt/tupi/HybridModeling/EasyDensity.jl/data/lucas_overlaid.csv\", DataFrame, normalizenames=true);\n",
    "println(size(df_o));\n",
    "\n",
    "############################\n",
    "###### clean targets #######\n",
    "############################\n",
    "\n",
    "# filter horizon depth = 10 cm\n",
    "df_o = df_o[df_o.hzn_dep .== 10, :];\n",
    "select!(df_o, Not(:hzn_dep));\n",
    "println(size(df_o))\n",
    "\n",
    "# identify noise time supervise\n",
    "gdf = groupby(df_o, :id);\n",
    "df_o.maxdiff = fill(0.0, nrow(df_o));  # initialize noise column\n",
    "# compute max abs difference of SOCconc per id\n",
    "for sub in groupby(df_o, :id)\n",
    "    soc = sort(sub.soc)\n",
    "\n",
    "    if length(soc) < 2\n",
    "        maxdiff = -1\n",
    "    else\n",
    "        maxdiff = maximum(abs.(diff(soc)))\n",
    "    end\n",
    "\n",
    "    df_o[df_o.id .== sub.id[1], :maxdiff] .= maxdiff\n",
    "    \n",
    "end\n",
    "println(size(df_o))\n",
    "df_o = df_o[df_o.maxdiff .<= 50, :];\n",
    "println(size(df_o))\n",
    "\n",
    "# coords = collect(zip(df_o.lat, df_o.lon));\n",
    "\n",
    "########################\n",
    "###### clean cov #######\n",
    "########################\n",
    "# t clean covariates\n",
    "names_cov = Symbol.(names(df_o))[18:end-1];\n",
    "\n",
    "# Fix soilsuite and cropland extent columns\n",
    "for col in names_cov\n",
    "    if occursin(\"_soilsuite_\", String(col))\n",
    "        df_o[!, col] = replace(df_o[!, col], missing => 0)\n",
    "    elseif occursin(\"cropland_extent_\", String(col))\n",
    "        df_o[!, col] = replace(df_o[!, col], missing => 0)\n",
    "        df_o[!, col] .= ifelse.(df_o[!, col] .> 0, 1, 0)\n",
    "    end\n",
    "end\n",
    "\n",
    "# rm missing values: 1. >5%, drop col; 2. <=5%, drop row\n",
    "cols_to_drop_row = Symbol[];\n",
    "cols_to_drop_col = Symbol[];\n",
    "for col in names_cov\n",
    "    n_missing = count(ismissing, df_o[!, col])\n",
    "    frac_missing = n_missing / nrow(df_o)\n",
    "    if frac_missing > 0.05\n",
    "        println(n_missing, \" \", col)\n",
    "        select!(df_o, Not(col))  # drop the column\n",
    "        push!(cols_to_drop_col, col)  \n",
    "    elseif n_missing > 0\n",
    "        # println(n_missing, \" \", col)\n",
    "        push!(cols_to_drop_row, col)  # collect column name\n",
    "    end\n",
    "\n",
    "    if occursin(\"CHELSA_kg\", String(col)) \n",
    "        push!(cols_to_drop_col, col) \n",
    "        select!(df_o, Not(col))  # rm kg catagorical col\n",
    "    end \n",
    "end\n",
    "\n",
    "names_cov = filter(x -> !(x in cols_to_drop_col), names_cov) # remove cols-to-drop from names_cov\n",
    "if !isempty(cols_to_drop_row) \n",
    "    df_o = subset(df_o, cols_to_drop_row .=> ByRow(!ismissing)) # drop rows with missing values in cols_to_drop_row\n",
    "end\n",
    "println(size(df_o))\n",
    "\n",
    "cols_to_drop_col = Symbol[] \n",
    "for col in names_cov\n",
    "    if std(df_o[:,col])==0\n",
    "        push!(cols_to_drop_col, col)  # rm constant col (std==0)\n",
    "        select!(df_o, Not(col))\n",
    "    end\n",
    "end\n",
    "names_cov = filter(x -> !(x in cols_to_drop_col), names_cov) # remove cols-to-drop from names_cov\n",
    "println(size(df_o))\n",
    "\n",
    "# for col in names_cov # to check covairate distribution\n",
    "#     println(string(col)[1:10], ' ', round(std(df[:, col]); digits=2), ' ', round(mean(df[:, col]); digits=2))\n",
    "# end\n",
    "\n",
    "# # Normalize covariates by (x-mean) / std\n",
    "means = map(c -> mean(skipmissing(df_o[!, c])), predictors);\n",
    "stds  = map(c -> std(skipmissing(df_o[!, c])), predictors);\n",
    "\n",
    "using JLD2\n",
    "\n",
    "scaler = Dict{Symbol, NamedTuple{(:mean, :std), Tuple{Float64, Float64}}}()\n",
    "\n",
    "for (i, col) in enumerate(predictors)\n",
    "    scaler[col] = (mean = means[i], std = stds[i])\n",
    "end\n",
    "\n",
    "cov_scaler = scaler\n",
    "@save \"./data/covs_scaler.jld2\" cov_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37df3f5-4410-47e2-874d-48557dfc32cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
